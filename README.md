# Optimizer-PyTorch
Package of Optimizer implemented with PyTorch .

### SGD: stochastic gradient descent

### Adam: A Method for Stochastic Optimization
 - https://arxiv.org/abs/1412.6980
 - https://openreview.net/forum?id=ryQu7f-RZ

### Adabound: Adaptive Gradient Methods with Dynamic Bound of Learning Rate
 - https://arxiv.org/abs/1902.09843
 - https://openreview.net/forum?id=Bkg3g2R9FX
 - https://github.com/Luolc/AdaBound

### RAdam: On the Variance of the Adaptive Learning Rate and Beyond
 - https://arxiv.org/abs/1908.03265
 - https://github.com/LiyuanLucasLiu/RAdam

### Lookahead: Lookahead Optimizer: k steps forward, 1 step back
 - https://arxiv.org/abs/1907.08610

### ExtraGradient


### Others
 - https://ruder.io/optimizing-gradient-descent/index.html
 - https://github.com/lifeiteng/Optimizers
 - http://stanford.edu/~boyd/
 - http://www.athenasc.com/nonlinbook.html